{"cells":[{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# reading data and inserting headers\n","headers = ['order_datetime','location','customer_name','order_items','amount','payment_type','card_number']\n","df = pd.read_csv('raw-data.csv', header=None, names=headers)\n","\n","# removing sensitive data (PII)\n","df.drop(columns=['customer_name','card_number'], inplace=True)\n","\n","# converting datetime into yyyy/mm/dd format\n","df['order_datetime'] = pd.to_datetime(df['order_datetime'])\n","\n","# removing rows with any empty fields\n","df = df.dropna(axis= 0)\n","\n","# creating unique index for every order and writing clean dataframe into new csv file\n","df.index.name = 'order_id'\n","df.to_csv('pd-clean-data.csv')\n","\n","# creating csv to show top 10 sales list\n","df = df.sort_values('amount', ascending=False)\n","df = df.head(10)\n","df.to_csv('pd-top-10.csv')"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import to_timestamp, monotonically_increasing_id\n","\n","# creating a SparkSession\n","spark = SparkSession.builder.appName(\"PySpark data cleaning\").getOrCreate()\n","\n","# defining schema and reading csv data\n","schema = 'order_datetime STRING, location STRING, customer_name STRING, order_items STRING, amount DOUBLE, payment_type STRING, card_number STRING'\n","df = spark.read.csv('raw-data.csv', schema=schema, header=False)\n","\n","# rename columns to match headers\n","headers = ['order_datetime', 'location', 'customer_name', 'order_items', 'amount', 'payment_type', 'card_number']\n","df = df.toDF(*headers)\n","\n","# removing sensitive data (PII)\n","df = df.drop('customer_name', 'card_number')\n","\n","# converting datetime into yyyy/mm/dd format\n","df = df.withColumn('order_datetime', to_timestamp('order_datetime', 'dd/MM/yyyy HH:mm'))\n","\n","# create unique index for every order\n","df = df.withColumn('order_id', monotonically_increasing_id())\n","\n","# writing clean DataFrame into new CSV file\n","new_file = 'pyspark-clean-data'\n","df.write.csv(new_file, header=True)\n","\n","# creating a top 10 sales list\n","df = df.orderBy(df['amount'].desc())\n","top_10_file = 'pyspark-top-10'\n","df.limit(10).write.csv(top_10_file, header=True)\n","\n","spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":2}
